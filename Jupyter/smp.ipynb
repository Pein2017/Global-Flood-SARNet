{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "import matplotlib.pyplot as plt\n",
    "# flake8: noqa: E501\n",
    "\n",
    "\n",
    "def visualize_s2_img(s2_channel_paths):\n",
    "    \"\"\"\n",
    "    Calculates and returns a 'SWIR' S2 false color composite ready for visualization.\n",
    "\n",
    "    Args:\n",
    "        s2_channel_paths (list of str): Paths to the ['B12', 'B7', 'B4'] bands of a S2 image.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Image (H, W, 3) ready for visualization with e.g. plt.imshow(..)\n",
    "    \"\"\"\n",
    "    img = []\n",
    "    for path in s2_channel_paths:\n",
    "        img.append(tifffile.imread(path))\n",
    "    img = np.stack(img, axis=-1)\n",
    "    return scale_S1_S2_img(img, sentinel=2)\n",
    "\n",
    "\n",
    "def visualize_s1_img(path_vv, path_vh):\n",
    "    \"\"\"\n",
    "    Calculates and returns a S1 false color composite ready for visualization.\n",
    "\n",
    "    Args:\n",
    "        path_vv (str): Path to the VV band.\n",
    "        path_vh (str): Path to the VH band.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Image (H, W, 3) ready for visualization with e.g. plt.imshow(..)\n",
    "    \"\"\"\n",
    "    s1_img = np.stack((tifffile.imread(path_vv), tifffile.imread(path_vh)),\n",
    "                      axis=-1)\n",
    "    img = np.zeros((s1_img.shape[0], s1_img.shape[1], 3), dtype=np.float32)\n",
    "    img[:, :, :2] = s1_img.copy()\n",
    "    img[:, :, 2] = s1_img[:, :, 0] / s1_img[:, :, 1]\n",
    "    s1_img = np.nan_to_num(s1_img)\n",
    "    return scale_S1_S2_img(img, sentinel=1)\n",
    "\n",
    "\n",
    "def scale_S1_S2_img(matrix, sentinel=2):\n",
    "    \"\"\"\n",
    "    Returns a scaled (H,W,D) image which is more easily visually inspectable. Image is linearly scaled between\n",
    "    min and max_value of by channel.\n",
    "    Args:\n",
    "        matrix (np.array): (H,W,D) image to be scaled\n",
    "        sentinel (int, optional): Sentinel 1 or Sentinel 2 image? Determines the min and max values for scalin, defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Image (H, W, 3) ready for visualization with e.g. plt.imshow(..)\n",
    "    \"\"\"\n",
    "    w, h, d = matrix.shape\n",
    "    min_values = np.array([100, 100, 100]) if sentinel == 2 else np.array(\n",
    "        [-23, -28, 0.2])\n",
    "    max_values = np.array([3500, 3500, 3500]) if sentinel == 2 else np.array(\n",
    "        [0, -5, 1])\n",
    "\n",
    "    matrix = np.reshape(matrix, [w * h, d]).astype(np.float64)\n",
    "    matrix = (matrix - min_values[None, :]) / (max_values[None, :] -\n",
    "                                               min_values[None, :])\n",
    "    matrix = np.reshape(matrix, [w, h, d])\n",
    "\n",
    "    matrix = matrix.clip(0, 1)\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def mask_to_img(label, color_dict):\n",
    "    \"\"\"Recodes a (H,W) mask to a (H,W,3) RGB image according to color_dict\"\"\"\n",
    "    mutually_exclusive = np.zeros(label.shape + (3, ), dtype=np.uint8)\n",
    "    for key in range(1, len(color_dict.keys()) + 1):\n",
    "        mutually_exclusive[label == key] = color_dict[key]\n",
    "    return mutually_exclusive\n",
    "\n",
    "\n",
    "class Sentinel1_Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_paths,\n",
    "        mask_paths,\n",
    "        transforms=None,\n",
    "        min_normalize=-77,\n",
    "        max_normalize=26,\n",
    "    ):\n",
    "        self.img_paths = img_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transforms = transforms\n",
    "        self.min_normalize = min_normalize\n",
    "        self.max_normalize = max_normalize\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load in image\n",
    "        arr_x = []\n",
    "        for path in self.img_paths[idx]:\n",
    "            arr_x.append(tifffile.imread(path))\n",
    "        arr_x = np.stack(arr_x, axis=-1)\n",
    "        # Min-Max Normalization\n",
    "        arr_x = np.clip(arr_x, self.min_normalize, self.max_normalize)\n",
    "        arr_x = (arr_x - self.min_normalize) / (self.max_normalize -\n",
    "                                                self.min_normalize)\n",
    "\n",
    "        sample = {\"image\": arr_x}\n",
    "\n",
    "        # Load in label mask\n",
    "        sample[\"mask\"] = tifffile.imread(self.mask_paths[idx])\n",
    "\n",
    "        # Apply Data Augmentation\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(image=sample[\"image\"],\n",
    "                                     mask=sample[\"mask\"])\n",
    "        if sample[\"image\"].shape[-1] < 20:\n",
    "            sample[\"image\"] = sample[\"image\"].transpose((2, 0, 1))\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def visualize(self, how_many=1, show_specific_index=None):\n",
    "        for _ in range(how_many):\n",
    "            rand_int = np.random.randint(len(self.img_paths))\n",
    "            if show_specific_index is not None:\n",
    "                rand_int = show_specific_index\n",
    "            print(self.img_paths[rand_int][0])\n",
    "            f, axarr = plt.subplots(1, 3, figsize=(30, 9))\n",
    "            axarr[0].imshow(\n",
    "                visualize_s1_img(self.img_paths[rand_int][0],\n",
    "                                 self.img_paths[rand_int][1]))\n",
    "            sample = self.__getitem__(rand_int)\n",
    "\n",
    "            img = sample[\"image\"]\n",
    "            axarr[0].set_title(\"FCC of original S1 image\", fontsize=15)  # noqa\n",
    "            axarr[1].imshow(img[0])  # Just visualize the VV band here\n",
    "            axarr[1].set_title(\n",
    "                f\"VV band returned from the dataset, Min: {img.min():.4f}, Max: {img.max():.4f}\",\n",
    "                fontsize=15)\n",
    "            if \"mask\" in sample.keys():\n",
    "                axarr[2].set_title(\n",
    "                    f\"Corresponding water mask: {(sample['mask'] == 1).sum()} px\",\n",
    "                    fontsize=15)\n",
    "                mask = mask_to_img(sample[\"mask\"], {1: (0, 0, 255)})\n",
    "                axarr[2].imshow(mask)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class XEDiceLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, alpha=0.5, EPS=1e-7):\n",
    "        super().__init__()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        self.alpha = alpha\n",
    "        self.EPS = EPS\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        xe_loss = self.cross_entropy(preds, targets)\n",
    "        dice_loss = self.calculate_dice_loss(preds, targets)\n",
    "        return self.alpha * xe_loss + (1 - self.alpha) * dice_loss\n",
    "\n",
    "    def calculate_dice_loss(self, preds, targets):\n",
    "        targets = targets.float()\n",
    "        preds = torch.softmax(preds, dim=1)[:, 1]\n",
    "        intersection = torch.sum(preds * targets)\n",
    "        union = torch.sum(preds + targets)\n",
    "        return 1 - (2.0 * intersection) / (union + self.EPS)\n",
    "\n",
    "\n",
    "# Metric Calculation\n",
    "def tp_fp_fn(preds, targets):\n",
    "    tp = torch.sum(preds * targets)\n",
    "    fp = torch.sum(preds) - tp\n",
    "    fn = torch.sum(targets) - tp\n",
    "    return tp.item(), fp.item(), fn.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import segmentation_models_pytorch as smp\n",
    "# flake8: noqa: E501\n",
    "\n",
    "\n",
    "class FloodwaterSegmentationModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_name=\"Unet\",  # 添加模型类型参数\n",
    "            encoder_name=\"resnet34\",\n",
    "            encoder_weights=\"imagenet\",\n",
    "            in_channels=2,\n",
    "            classes=2):\n",
    "        \"\"\"\n",
    "        初始化 FloodwaterSegmentationModel 类。\n",
    "\n",
    "        Args:\n",
    "            model_name (str): 选择的模型类型，如 'Unet', 'Linknet', 'FPN', 'DeepLabV3' 等。\n",
    "            encoder_name (str): 使用的编码器名称。默认为 'resnet34'。\n",
    "            encoder_weights (str): 编码器的预训练权重。默认为 'imagenet'。\n",
    "            in_channels (int): 输入通道数。对于 Sentinel-1 数据，默认为 2 (VV 和 VH)。\n",
    "            classes (int): 输出类别数。对于二分类问题，默认为 2。\n",
    "        \"\"\"\n",
    "        super(FloodwaterSegmentationModel, self).__init__()\n",
    "\n",
    "        # 根据模型类型选择相应的模型\n",
    "        if model_name == \"Unet\":\n",
    "            self.model = smp.Unet(encoder_name=encoder_name,\n",
    "                                  encoder_weights=encoder_weights,\n",
    "                                  in_channels=in_channels,\n",
    "                                  classes=classes)\n",
    "        elif model_name == \"Linknet\":\n",
    "            self.model = smp.Linknet(encoder_name=encoder_name,\n",
    "                                     encoder_weights=encoder_weights,\n",
    "                                     in_channels=in_channels,\n",
    "                                     classes=classes)\n",
    "        elif model_name == \"FPN\":\n",
    "            self.model = smp.FPN(encoder_name=encoder_name,\n",
    "                                 encoder_weights=encoder_weights,\n",
    "                                 in_channels=in_channels,\n",
    "                                 classes=classes)\n",
    "        elif model_name == \"DeepLabV3\":\n",
    "            self.model = smp.DeepLabV3(encoder_name=encoder_name,\n",
    "                                       encoder_weights=encoder_weights,\n",
    "                                       in_channels=in_channels,\n",
    "                                       classes=classes)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {model_name}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        定义模型的前向传播。\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): 输入数据。\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: 模型的输出。\n",
    "        \"\"\"\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import segmentation_models_pytorch as smp\n",
    "# flake8: noqa: E501\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    Adapted from https://github.com/pytorch/examples/blob/master/imagenet/main.py    # noqa: E501\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    if device.startswith('npu'):\n",
    "        return {k: v.to(device, non_blocking=True) for k, v in data.items()}\n",
    "    else:\n",
    "        Warning(\"NPU Device is not supported\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def print_epoch_stats(phase,\n",
    "                      epoch,\n",
    "                      loss_meter,\n",
    "                      batch_time,\n",
    "                      data_time,\n",
    "                      start_time,\n",
    "                      iou=None):\n",
    "    if iou:\n",
    "        print(\n",
    "            f\"Epoch: [{epoch}] {phase} - TotalT: {(time.time() - start_time) / 60:.1f} min, \"\n",
    "            f\"Loss: {loss_meter.avg:.4f}\")\n",
    "        print(f\"Global IoU: {iou:.4f}\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"Epoch: [{epoch}] {phase} - TotalT: {(time.time() - start_time) / 60:.1f} min, \"\n",
    "            f\"BatchT: {batch_time.avg:.3f}s, DataT: {data_time.avg:.3f}s, Loss: {loss_meter.avg:.4f}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def save_model(save_dict, log_path):\n",
    "    torch.save(save_dict, f\"{log_path}/best_model.pt\")\n",
    "\n",
    "\n",
    "# def save_model(epoch, iou, model, optimizer, experiment_name, log_path,\n",
    "#                best_metric, best_metric_epoch):\n",
    "\n",
    "#     save_dict = {\n",
    "#         \"model_name\": experiment_name,\n",
    "#         \"epoch_num\": epoch,\n",
    "#         \"model_state_dict\": model.state_dict(),\n",
    "#         \"optim_state_dict\": optimizer.state_dict(),\n",
    "#         \"iou\": iou\n",
    "#     }\n",
    "#     old_model_path = glob.glob(f\"{log_path}/best_iou*\")\n",
    "#     if old_model_path:\n",
    "#         os.remove(old_model_path[0])\n",
    "#     torch.save(save_dict, f\"{log_path}/best_iou_{epoch}_{iou:.4f}.pt\")\n",
    "\n",
    "#     if iou > best_metric:\n",
    "#         best_metric, best_metric_epoch = iou, epoch\n",
    "\n",
    "#     return best_metric, best_metric_epoch\n",
    "\n",
    "\n",
    "def create_data_loader(dataset, batch_size, shuffle, num_workers, pin_memory):\n",
    "    return torch.utils.data.DataLoader(dataset,\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=shuffle,\n",
    "                                       num_workers=num_workers,\n",
    "                                       pin_memory=pin_memory)\n",
    "\n",
    "\n",
    "def generate_paths(base_path, pattern, replacements):\n",
    "    paths = sorted(glob.glob(f\"{base_path}/{pattern}\"))\n",
    "    return [\n",
    "        list(map(lambda r: path.replace(*r), replacements)) for path in paths\n",
    "    ]\n",
    "\n",
    "\n",
    "def split_data(paths, train_perc, val_perc):\n",
    "    np.random.seed(17)\n",
    "    np.random.shuffle(paths)\n",
    "    n = len(paths)\n",
    "    return {\n",
    "        \"train\": paths[:int(train_perc * n)],\n",
    "        \"val\": paths[int(train_perc * n):int((train_perc + val_perc) * n)],\n",
    "        \"test\": paths[int((train_perc + val_perc) * n):]\n",
    "    }\n",
    "\n",
    "\n",
    "def download_smp_model(model_params):\n",
    "    model = getattr(smp, model_params[\"model_name\"])(\n",
    "        encoder_name=model_params[\"encoder_name\"],\n",
    "        encoder_weights=model_params[\"encoder_weights\"],\n",
    "        in_channels=model_params[\"in_channels\"],\n",
    "        classes=model_params[\"classes\"])\n",
    "    print(\n",
    "        f\"Downloaded and cached {model_params['model_name']} model with {model_params['encoder_name']} encoder.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from loss_metrics import tp_fp_fn\n",
    "# from utils import AverageMeter, print_epoch_stats, to_device, save_model\n",
    "import time\n",
    "import torch\n",
    "\n",
    "\n",
    "def train_epochs(model,\n",
    "                 train_loader,\n",
    "                 val_loader,\n",
    "                 optimizer,\n",
    "                 loss_func,\n",
    "                 experiment_name,\n",
    "                 log_path,\n",
    "                 scheduler,\n",
    "                 N_EPOCHS,\n",
    "                 EARLY_STOP_THRESHOLD,\n",
    "                 EARLY_STOP_PATIENCE,\n",
    "                 DEVICE,\n",
    "                 EPS,\n",
    "                 start_epoch=1,\n",
    "                 best_metric=0,\n",
    "                 best_metric_epoch=0):\n",
    "    best_metric, best_metric_epoch = 0, 0\n",
    "    early_stop_counter, best_val_early_stopping_metric = 0, 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for curr_epoch_num in range(start_epoch, N_EPOCHS + 1):\n",
    "        # 训练\n",
    "        model.train()\n",
    "        train_loss, batch_time, data_time = AverageMeter(), AverageMeter(\n",
    "        ), AverageMeter()\n",
    "        end = time.time()\n",
    "\n",
    "        for data in train_loader:\n",
    "            data_time.update(time.time() - end)\n",
    "            data = to_device(data, DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(data[\"image\"])\n",
    "            loss = loss_func(preds, data[\"mask\"].long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.update(loss.item(), data[\"image\"].size(0))\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "        print_epoch_stats('Train', curr_epoch_num, train_loss, batch_time,\n",
    "                          data_time, start_time)\n",
    "\n",
    "        # 验证\n",
    "        model.eval()\n",
    "        val_loss, tps, fps, fns = AverageMeter(), 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                data = to_device(data, DEVICE)\n",
    "                preds = model(data[\"image\"])\n",
    "                loss = loss_func(preds, data[\"mask\"].long())\n",
    "                val_loss.update(loss.item(), data[\"image\"].size(0))\n",
    "                preds_binary = (torch.softmax(preds, dim=1)[:, 1] > 0.5).long()\n",
    "                tp, fp, fn = tp_fp_fn(preds_binary, data[\"mask\"])\n",
    "                tps += tp\n",
    "                fps += fp\n",
    "                fns += fn\n",
    "\n",
    "        iou_global = tps / (tps + fps + fns + EPS)\n",
    "        print_epoch_stats('Val', curr_epoch_num, val_loss, batch_time,\n",
    "                          data_time, start_time, iou_global)\n",
    "\n",
    "        # 保存Scheduler和Model\n",
    "        if curr_epoch_num > EARLY_STOP_THRESHOLD:\n",
    "            scheduler.step(iou_global)\n",
    "        if iou_global > best_metric:\n",
    "            best_metric, best_metric_epoch = iou_global, curr_epoch_num\n",
    "            best_model_state = {\n",
    "                \"model_name\": experiment_name,\n",
    "                \"epoch_num\": curr_epoch_num,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optim_state_dict\": optimizer.state_dict(),\n",
    "                \"iou\": iou_global\n",
    "            }\n",
    "        if iou_global < best_val_early_stopping_metric:\n",
    "            early_stop_counter += 1\n",
    "        else:\n",
    "            best_val_early_stopping_metric, early_stop_counter = iou_global, 0\n",
    "        if early_stop_counter > EARLY_STOP_PATIENCE:\n",
    "            print(\"Early Stopping\")\n",
    "            break\n",
    "\n",
    "    # 训练结束后保存最佳模型\n",
    "    if best_model_state:\n",
    "        save_model(best_model_state, log_path)\n",
    "\n",
    "    print(f\"Best IOU: {best_metric} at Epoch: {best_metric_epoch}\")\n",
    "\n",
    "    # 返回有用的信息\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"optimizer\": optimizer,\n",
    "        \"scheduler\": scheduler,\n",
    "        \"best_metric\": best_metric,\n",
    "        \"best_metric_epoch\": best_metric_epoch,\n",
    "        \"total_epochs\": curr_epoch_num,\n",
    "        \"training_time\": time.time() - start_time\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_npu\n",
    "import albumentations\n",
    "# flake8: noqa\n",
    "\n",
    "# 数据集和路径相关\n",
    "BASE_PATH = \"/home/HW/Pein/Floodwater/dataset\"\n",
    "TRAIN_PERCENTAGE = 0.8\n",
    "VAL_PERCENTAGE = 0.1\n",
    "TEST_PERCENTAGE = round(1 - TRAIN_PERCENTAGE - VAL_PERCENTAGE, 2)\n",
    "\n",
    "# 数据预处理和增强\n",
    "MIN_NORMALIZE = -77\n",
    "MAX_NORMALIZE = 26\n",
    "TRAIN_CROP_SIZE = 256\n",
    "TARGET_SIZE = 256\n",
    "\n",
    "# 数据增强配置\n",
    "train_transforms = albumentations.Compose([\n",
    "    albumentations.RandomCrop(TRAIN_CROP_SIZE, TRAIN_CROP_SIZE),\n",
    "    # RandomResizedCrop(TARGET_SIZE, TARGET_SIZE, scale=(0.75, 1.0), p=0.5),\n",
    "    albumentations.RandomBrightnessContrast(brightness_limit=0.2,\n",
    "                                            contrast_limit=0.2),\n",
    "    albumentations.GaussNoise(var_limit=(10, 50)),\n",
    "    albumentations.RandomRotate90(),\n",
    "    albumentations.HorizontalFlip(),\n",
    "    albumentations.VerticalFlip(),\n",
    "    albumentations.Resize(TARGET_SIZE, TARGET_SIZE),\n",
    "])\n",
    "\n",
    "# 模型和训练相关参数\n",
    "NUM_WORKERS = 4\n",
    "PIN_MEMORY = False\n",
    "BATCH_SIZE = 128\n",
    "EPS = 1e-7\n",
    "\n",
    "PATIENCE = 6\n",
    "N_EPOCHS = 200\n",
    "LEARNING_RATE = 1e-3\n",
    "EARLY_STOP_THRESHOLD = 10\n",
    "EARLY_STOP_PATIENCE = PATIENCE * 5\n",
    "\n",
    "# 模型参数\n",
    "MODEL_PARAMS = {\n",
    "    \"model_name\": \"Unet\",\n",
    "    \"encoder_name\": \"resnet50\",\n",
    "    \"encoder_weights\": \"imagenet\",\n",
    "    \"in_channels\": 2,\n",
    "    \"classes\": 2\n",
    "}\n",
    "\n",
    "# 设备和日志相关\n",
    "DEVICE = 'npu:0' if torch.npu.is_available() else 'cpu'\n",
    "EXPERIMENT_NAME = f\"Exp3-{MODEL_PARAMS['model_name']}-{MODEL_PARAMS['encoder_name']}\"\n",
    "LOG_PATH = f\"/home/HW/Pein/Floodwater/SMP/trained_models/{EXPERIMENT_NAME}\"\n",
    "\n",
    "\n",
    "# 训练模式：'new' 从头开始，'continue' 从checkpoint继续\n",
    "TRAIN_MODE = 'new'  # 或 'continue'\n",
    "\n",
    "# 如果是 'tuning' 模式，指定checkpoint路径\n",
    "CHECKPOINT_PATH = '/home/HW/Pein/Floodwater/trained_models/Exp3-Unet-resnet50-npu:0/best_model.pt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and cached Unet model with resnet50 encoder.\n",
      "Using NPU: npu:0 ...\n",
      "\n",
      "TRAIN_CROP_SIZE = 256\n",
      "TARGET_SIZE = 256\n",
      "NUM_WORKERS = 4\n",
      "PIN_MEMORY = False\n",
      "BATCH_SIZE = 128\n",
      "EPS = 1e-07\n",
      "EXPERIMENT_NAME = 'Exp3-Unet-resnet50'\n",
      "ENCODER_NAME = 'resnet50'\n",
      "MODEL_NAME = 'Unet'\n",
      "ENCODER_WEIGHTS = 'imagenet'\n",
      "IN_CHANNELS = 2\n",
      "CLASSES = 2\n",
      "PATIENCE = 6\n",
      "N_EPOCHS = 200\n",
      "LEARNING_RATE = 0.001\n",
      "EARLY_STOP_THRESHOLD = 10\n",
      "EARLY_STOP_PATIENCE = 30\n",
      "\n",
      "Training from scratch ...\n"
     ]
    }
   ],
   "source": [
    "import torch_npu\n",
    "import torch\n",
    "import albumentations\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import sys\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import segmentation_models_pytorch as smp\n",
    "import time\n",
    "# from utils import (AverageMeter, print_epoch_stats, save_model, to_device,\n",
    "#                    create_data_loader, generate_paths, split_data,\n",
    "#                    download_smp_model)\n",
    "# from data_processing import Sentinel1_Dataset\n",
    "# from loss_metrics import XEDiceLoss, tp_fp_fn\n",
    "# from train import train_epochs\n",
    "# from model import FloodwaterSegmentationModel\n",
    "# from config import *\n",
    "# flake8: noqa\n",
    "\n",
    "# 定义文件路径替换\n",
    "label_replacements = [(\"LabelWater.tif\", \"LabelWater.tif\")]\n",
    "image_replacements = [(\"LabelWater.tif\", \"VV.tif\"),\n",
    "                      (\"LabelWater.tif\", \"VH.tif\")]\n",
    "\n",
    "# 生成文件路径\n",
    "label_paths = generate_paths(BASE_PATH, \"chips/*/s1/*/LabelWater.tif\",\n",
    "                             label_replacements)\n",
    "image_paths = generate_paths(BASE_PATH, \"chips/*/s1/*/LabelWater.tif\",\n",
    "                             image_replacements)\n",
    "\n",
    "# 数据拆分\n",
    "label_splits = split_data(label_paths, TRAIN_PERCENTAGE, VAL_PERCENTAGE)\n",
    "image_splits = split_data(image_paths, TRAIN_PERCENTAGE, VAL_PERCENTAGE)\n",
    "\n",
    "# Sentinel1_Dataset初始化\n",
    "train_dataset = Sentinel1_Dataset(image_splits['train'],\n",
    "                                  label_splits['train'],\n",
    "                                  transforms=train_transforms,\n",
    "                                  min_normalize=MIN_NORMALIZE,\n",
    "                                  max_normalize=MAX_NORMALIZE)\n",
    "val_dataset = Sentinel1_Dataset(image_splits['val'],\n",
    "                                label_splits['val'],\n",
    "                                transforms=None,\n",
    "                                min_normalize=MIN_NORMALIZE,\n",
    "                                max_normalize=MAX_NORMALIZE)\n",
    "test_dataset = Sentinel1_Dataset(image_splits['test'],\n",
    "                                 label_splits['test'],\n",
    "                                 transforms=None,\n",
    "                                 min_normalize=MIN_NORMALIZE,\n",
    "                                 max_normalize=MAX_NORMALIZE)\n",
    "\n",
    "# 其他设置（NPU、文件夹创建、模型下载等）\n",
    "os.makedirs(LOG_PATH, exist_ok=True)\n",
    "download_smp_model(MODEL_PARAMS)\n",
    "torch.npu.set_device(DEVICE)\n",
    "\n",
    "#NPU设置\n",
    "torch.npu.set_device(DEVICE)\n",
    "if torch.npu.is_available() and DEVICE.startswith('npu'):\n",
    "    print(f'Using NPU: {DEVICE} ...')\n",
    "else:\n",
    "    print('Using CPU ...')\n",
    "\n",
    "# 打印基本信息\n",
    "print(f\"\"\"\n",
    "TRAIN_CROP_SIZE = {TRAIN_CROP_SIZE}\n",
    "TARGET_SIZE = {TARGET_SIZE}\n",
    "NUM_WORKERS = {NUM_WORKERS}\n",
    "PIN_MEMORY = {PIN_MEMORY}\n",
    "BATCH_SIZE = {BATCH_SIZE}\n",
    "EPS = {EPS}\n",
    "EXPERIMENT_NAME = '{EXPERIMENT_NAME}'\n",
    "ENCODER_NAME = '{MODEL_PARAMS['encoder_name']}'\n",
    "MODEL_NAME = '{MODEL_PARAMS['model_name']}'\n",
    "ENCODER_WEIGHTS = '{MODEL_PARAMS['encoder_weights']}'\n",
    "IN_CHANNELS = {MODEL_PARAMS['in_channels']}\n",
    "CLASSES = {MODEL_PARAMS['classes']}\n",
    "PATIENCE = {PATIENCE}\n",
    "N_EPOCHS = {N_EPOCHS}\n",
    "LEARNING_RATE = {LEARNING_RATE}\n",
    "EARLY_STOP_THRESHOLD = {EARLY_STOP_THRESHOLD}\n",
    "EARLY_STOP_PATIENCE = {EARLY_STOP_PATIENCE}\n",
    "\"\"\")\n",
    "\n",
    "# 加载数据\n",
    "train_loader = create_data_loader(train_dataset,\n",
    "                                  BATCH_SIZE,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=NUM_WORKERS,\n",
    "                                  pin_memory=PIN_MEMORY)\n",
    "val_loader = create_data_loader(val_dataset,\n",
    "                                BATCH_SIZE,\n",
    "                                shuffle=False,\n",
    "                                num_workers=NUM_WORKERS,\n",
    "                                pin_memory=PIN_MEMORY)\n",
    "test_loader = create_data_loader(test_dataset,\n",
    "                                 BATCH_SIZE,\n",
    "                                 shuffle=False,\n",
    "                                 num_workers=NUM_WORKERS,\n",
    "                                 pin_memory=PIN_MEMORY)\n",
    "\n",
    "# 模型初始化\n",
    "loss_func = XEDiceLoss().to(DEVICE)\n",
    "model = FloodwaterSegmentationModel(**MODEL_PARAMS).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=LEARNING_RATE if TRAIN_MODE == 'new' else None)\n",
    "\n",
    "if TRAIN_MODE == 'continue':\n",
    "    # 从checkpoint加载模型和优化器状态\n",
    "    model, optimizer, best_metric, best_metric_epoch = load_model(\n",
    "        CHECKPOINT_PATH, model, optimizer)\n",
    "    print('Loaded model and optimizer from checkpoint ...')\n",
    "    print(\n",
    "        f\"previous learning rate: {optimizer.param_groups[0]['lr']} , best IOU: {best_metric} at epoch: {best_metric_epoch}\"\n",
    "    )\n",
    "\n",
    "    # optional：修改学习率\n",
    "    # optimizer.param_groups[0]['lr'] = 1e-5\n",
    "else:\n",
    "    # 从头开始训练，初始化训练指标\n",
    "    print('Training from scratch ...')\n",
    "    best_metric, best_metric_epoch = 0, 0\n",
    "\n",
    "# 初始化学习率调度器\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                 mode=\"min\",\n",
    "                                                 factor=0.1,\n",
    "                                                 patience=PATIENCE,\n",
    "                                                 verbose=True)\n",
    "\n",
    "# 训练指标初始化\n",
    "early_stop_counter, best_val_early_stopping_metric = 0, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1] Train - TotalT: 12.4 min, BatchT: 123.457s, DataT: 0.862s, Loss: 0.7687\n",
      "Epoch: [1] Val - TotalT: 15.0 min, Loss: 31.0995, Global IoU: 0.0019\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "NPU out of memory. Tried to allocate 130.00 MiB (NPU 0; 32.00 GiB total capacity; 16.83 GiB already allocated; 156.81 MiB free; 17.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/HW/Pein/Floodwater/Jupyter/smp.ipynb 单元格 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22424d535f31227d/home/HW/Pein/Floodwater/Jupyter/smp.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 开始训练\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22424d535f31227d/home/HW/Pein/Floodwater/Jupyter/smp.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m train_result \u001b[39m=\u001b[39m train_epochs(model, train_loader, val_loader, optimizer,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22424d535f31227d/home/HW/Pein/Floodwater/Jupyter/smp.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m                             loss_func, EXPERIMENT_NAME, LOG_PATH, scheduler,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22424d535f31227d/home/HW/Pein/Floodwater/Jupyter/smp.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m                             N_EPOCHS, EARLY_STOP_THRESHOLD,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22424d535f31227d/home/HW/Pein/Floodwater/Jupyter/smp.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m                             EARLY_STOP_PATIENCE, DEVICE, EPS)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22424d535f31227d/home/HW/Pein/Floodwater/Jupyter/smp.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# 打印或处理返回的信息\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22424d535f31227d/home/HW/Pein/Floodwater/Jupyter/smp.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m--\u001b[39m\u001b[39m'\u001b[39m \u001b[39m*\u001b[39m \u001b[39m20\u001b[39m)\n",
      "\u001b[1;32m/home/HW/Pein/Floodwater/Jupyter/smp.ipynb 单元格 15\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22424d535f31227d/home/HW/Pein/Floodwater/Jupyter/smp.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m data \u001b[39m=\u001b[39m to_device(data, DEVICE)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22424d535f31227d/home/HW/Pein/Floodwater/Jupyter/smp.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22424d535f31227d/home/HW/Pein/Floodwater/Jupyter/smp.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m preds \u001b[39m=\u001b[39m model(data[\u001b[39m\"\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22424d535f31227d/home/HW/Pein/Floodwater/Jupyter/smp.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_func(preds, data[\u001b[39m\"\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mlong())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22424d535f31227d/home/HW/Pein/Floodwater/Jupyter/smp.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/Pein/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/HW/Pein/Floodwater/Jupyter/smp.ipynb 单元格 15\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22424d535f31227d/home/HW/Pein/Floodwater/Jupyter/smp.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22424d535f31227d/home/HW/Pein/Floodwater/Jupyter/smp.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22424d535f31227d/home/HW/Pein/Floodwater/Jupyter/smp.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m    定义模型的前向传播。\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22424d535f31227d/home/HW/Pein/Floodwater/Jupyter/smp.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22424d535f31227d/home/HW/Pein/Floodwater/Jupyter/smp.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m        torch.Tensor: 模型的输出。\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22424d535f31227d/home/HW/Pein/Floodwater/Jupyter/smp.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22424d535f31227d/home/HW/Pein/Floodwater/Jupyter/smp.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/Pein/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/Pein/lib/python3.9/site-packages/segmentation_models_pytorch/base/model.py:30\u001b[0m, in \u001b[0;36mSegmentationModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_input_shape(x)\n\u001b[1;32m     29\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(x)\n\u001b[0;32m---> 30\u001b[0m decoder_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\u001b[39m*\u001b[39;49mfeatures)\n\u001b[1;32m     32\u001b[0m masks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msegmentation_head(decoder_output)\n\u001b[1;32m     34\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassification_head \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/Pein/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/Pein/lib/python3.9/site-packages/segmentation_models_pytorch/decoders/unet/decoder.py:119\u001b[0m, in \u001b[0;36mUnetDecoder.forward\u001b[0;34m(self, *features)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mfor\u001b[39;00m i, decoder_block \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks):\n\u001b[1;32m    118\u001b[0m     skip \u001b[39m=\u001b[39m skips[i] \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(skips) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     x \u001b[39m=\u001b[39m decoder_block(x, skip)\n\u001b[1;32m    121\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/Pein/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/Pein/lib/python3.9/site-packages/segmentation_models_pytorch/decoders/unet/decoder.py:41\u001b[0m, in \u001b[0;36mDecoderBlock.forward\u001b[0;34m(self, x, skip)\u001b[0m\n\u001b[1;32m     39\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention1(x)\n\u001b[1;32m     40\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n\u001b[0;32m---> 41\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2(x)\n\u001b[1;32m     42\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention2(x)\n\u001b[1;32m     43\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/Pein/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/Pein/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/Pein/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/Pein/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/Pein/lib/python3.9/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: NPU out of memory. Tried to allocate 130.00 MiB (NPU 0; 32.00 GiB total capacity; 16.83 GiB already allocated; 156.81 MiB free; 17.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation."
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "train_result = train_epochs(model, train_loader, val_loader, optimizer,\n",
    "                            loss_func, EXPERIMENT_NAME, LOG_PATH, scheduler,\n",
    "                            N_EPOCHS, EARLY_STOP_THRESHOLD,\n",
    "                            EARLY_STOP_PATIENCE, DEVICE, EPS)\n",
    "\n",
    "# 打印或处理返回的信息\n",
    "print('--' * 20)\n",
    "print(\"Training Completed:\")\n",
    "print(f\"Total Training Time: {train_result['training_time']} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pein",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
