Downloaded and cached PAN model with resnet101 encoder.
Using NPU: npu:2 ...

TRAIN_CROP_SIZE = 256
TARGET_SIZE = 256
NUM_WORKERS = 4
PIN_MEMORY = False
BATCH_SIZE = 16
EPS = 1e-07
EXPERIMENT_NAME = 'PAN-resnet101-b16'
ENCODER_NAME = 'resnet101'
MODEL_NAME = 'PAN'
ENCODER_WEIGHTS = 'imagenet'
IN_CHANNELS = 2
CLASSES = 2
PATIENCE = 6
N_EPOCHS = 200
LEARNING_RATE = 0.0001
EARLY_STOP_THRESHOLD = 10
EARLY_STOP_PATIENCE = 30

-\|/-\|/-\Epoch: [1] Train - EpochT: 1.9 min, BatchT: 2.501s, DataT: 0.021s, Loss: 0.6205
|/-\|/-\|/-\|/-\|/-\|/-\|/-Epoch: [1] Val - EpochT: 6.4 min, Loss: 0.4083
Global IOU: 0.5147
Epoch: [2] Train - EpochT: 0.1 min, BatchT: 0.194s, DataT: 0.024s, Loss: 0.5049
Epoch: [2] Val - EpochT: 0.2 min, Loss: 0.2591
Global IOU: 0.6798
Epoch: [3] Train - EpochT: 0.1 min, BatchT: 0.190s, DataT: 0.023s, Loss: 0.4818
Epoch: [3] Val - EpochT: 0.2 min, Loss: 0.2950
Global IOU: 0.6338
Epoch: [4] Train - EpochT: 0.1 min, BatchT: 0.187s, DataT: 0.020s, Loss: 0.4826
Epoch: [4] Val - EpochT: 0.2 min, Loss: 0.2457
Global IOU: 0.6897
Epoch: [5] Train - EpochT: 0.1 min, BatchT: 0.192s, DataT: 0.025s, Loss: 0.4736
Epoch: [5] Val - EpochT: 0.2 min, Loss: 0.1991
Global IOU: 0.7327
Epoch: [6] Train - EpochT: 0.1 min, BatchT: 0.193s, DataT: 0.025s, Loss: 0.4683
Epoch: [6] Val - EpochT: 0.2 min, Loss: 0.2149
Global IOU: 0.7171
Epoch: [7] Train - EpochT: 0.2 min, BatchT: 0.200s, DataT: 0.024s, Loss: 0.4631
Epoch: [7] Val - EpochT: 0.2 min, Loss: 0.1957
Global IOU: 0.7368
Epoch: [8] Train - EpochT: 0.1 min, BatchT: 0.190s, DataT: 0.023s, Loss: 0.4618
Epoch: [8] Val - EpochT: 0.2 min, Loss: 0.3490
Global IOU: 0.4788
Epoch: [9] Train - EpochT: 0.1 min, BatchT: 0.193s, DataT: 0.024s, Loss: 0.4471
Epoch: [9] Val - EpochT: 0.2 min, Loss: 0.1664
Global IOU: 0.7482
Epoch: [10] Train - EpochT: 0.1 min, BatchT: 0.190s, DataT: 0.022s, Loss: 0.4102
Epoch: [10] Val - EpochT: 0.2 min, Loss: 0.1752
Global IOU: 0.7547
Epoch: [11] Train - EpochT: 0.2 min, BatchT: 0.208s, DataT: 0.022s, Loss: 0.4560
Epoch: [11] Val - EpochT: 0.2 min, Loss: 0.1688
Global IOU: 0.7630
Epoch: [12] Train - EpochT: 0.1 min, BatchT: 0.193s, DataT: 0.025s, Loss: 0.4553
Epoch: [12] Val - EpochT: 0.2 min, Loss: 0.2154
Global IOU: 0.7122
Epoch: [13] Train - EpochT: 0.1 min, BatchT: 0.195s, DataT: 0.023s, Loss: 0.4218
Epoch: [13] Val - EpochT: 0.2 min, Loss: 0.1621
Global IOU: 0.7748
Epoch: [14] Train - EpochT: 0.1 min, BatchT: 0.193s, DataT: 0.025s, Loss: 0.4319
Epoch: [14] Val - EpochT: 0.2 min, Loss: 0.1616
Global IOU: 0.7682
Epoch: [15] Train - EpochT: 0.1 min, BatchT: 0.193s, DataT: 0.023s, Loss: 0.4474
Epoch: [15] Val - EpochT: 0.2 min, Loss: 0.1647
Global IOU: 0.7586
Epoch: [16] Train - EpochT: 0.1 min, BatchT: 0.191s, DataT: 0.022s, Loss: 0.4144
Epoch: [16] Val - EpochT: 0.2 min, Loss: 0.1515
Global IOU: 0.7648
Epoch: [17] Train - EpochT: 0.1 min, BatchT: 0.189s, DataT: 0.023s, Loss: 0.4197
Epoch: [17] Val - EpochT: 0.2 min, Loss: 0.1840
Global IOU: 0.7593
Epoch: [18] Train - EpochT: 0.1 min, BatchT: 0.194s, DataT: 0.024s, Loss: 0.4004
Epoch: [18] Val - EpochT: 0.2 min, Loss: 0.1383
Global IOU: 0.7855
Epoch: [19] Train - EpochT: 0.1 min, BatchT: 0.191s, DataT: 0.025s, Loss: 0.4284
Epoch: [19] Val - EpochT: 0.2 min, Loss: 0.1343
Global IOU: 0.7954
Epoch 00009: reducing learning rate of group 0 to 5.0000e-05.
Epoch: [20] Train - EpochT: 0.1 min, BatchT: 0.190s, DataT: 0.022s, Loss: 0.4329
Epoch: [20] Val - EpochT: 0.2 min, Loss: 0.1563
Global IOU: 0.7688
Epoch: [21] Train - EpochT: 0.1 min, BatchT: 0.192s, DataT: 0.025s, Loss: 0.4441
Epoch: [21] Val - EpochT: 0.2 min, Loss: 0.1528
Global IOU: 0.7709
Epoch: [22] Train - EpochT: 0.1 min, BatchT: 0.192s, DataT: 0.022s, Loss: 0.3969
Epoch: [22] Val - EpochT: 0.2 min, Loss: 0.1479
Global IOU: 0.7615
Epoch: [23] Train - EpochT: 0.1 min, BatchT: 0.192s, DataT: 0.025s, Loss: 0.4361
Epoch: [23] Val - EpochT: 0.2 min, Loss: 0.1457
Global IOU: 0.7861
Epoch: [24] Train - EpochT: 0.2 min, BatchT: 0.198s, DataT: 0.023s, Loss: 0.4045
Epoch: [24] Val - EpochT: 0.2 min, Loss: 0.1397
Global IOU: 0.7932
Epoch: [25] Train - EpochT: 0.1 min, BatchT: 0.190s, DataT: 0.022s, Loss: 0.3826
Epoch: [25] Val - EpochT: 0.2 min, Loss: 0.1400
Global IOU: 0.7876
Epoch: [26] Train - EpochT: 0.1 min, BatchT: 0.193s, DataT: 0.023s, Loss: 0.4092
Epoch: [26] Val - EpochT: 0.2 min, Loss: 0.1485
Global IOU: 0.7794
Epoch 00016: reducing learning rate of group 0 to 2.5000e-05.
Epoch: [27] Train - EpochT: 0.1 min, BatchT: 0.190s, DataT: 0.024s, Loss: 0.3777
Epoch: [27] Val - EpochT: 0.2 min, Loss: 0.1273
Global IOU: 0.8169
Epoch: [28] Train - EpochT: 0.1 min, BatchT: 0.188s, DataT: 0.021s, Loss: 0.3669
Epoch: [28] Val - EpochT: 0.2 min, Loss: 0.1238
Global IOU: 0.8139
Epoch: [29] Train - EpochT: 0.1 min, BatchT: 0.191s, DataT: 0.023s, Loss: 0.3884
Epoch: [29] Val - EpochT: 0.2 min, Loss: 0.1264
Global IOU: 0.8117
Epoch: [30] Train - EpochT: 0.1 min, BatchT: 0.191s, DataT: 0.025s, Loss: 0.4115
Epoch: [30] Val - EpochT: 0.2 min, Loss: 0.1208
Global IOU: 0.8227
Epoch: [31] Train - EpochT: 0.1 min, BatchT: 0.189s, DataT: 0.024s, Loss: 0.3912
Epoch: [31] Val - EpochT: 0.2 min, Loss: 0.1294
Global IOU: 0.8074
Epoch: [32] Train - EpochT: 0.2 min, BatchT: 0.197s, DataT: 0.029s, Loss: 0.3707
Epoch: [32] Val - EpochT: 0.2 min, Loss: 0.1257
Global IOU: 0.8185
Epoch: [33] Train - EpochT: 0.1 min, BatchT: 0.188s, DataT: 0.023s, Loss: 0.3897
Epoch: [33] Val - EpochT: 0.2 min, Loss: 0.1172
Global IOU: 0.8363
Epoch 00023: reducing learning rate of group 0 to 1.2500e-05.
Epoch: [34] Train - EpochT: 0.1 min, BatchT: 0.192s, DataT: 0.024s, Loss: 0.4069
Epoch: [34] Val - EpochT: 0.2 min, Loss: 0.1186
Global IOU: 0.8234
Epoch: [35] Train - EpochT: 0.1 min, BatchT: 0.193s, DataT: 0.024s, Loss: 0.3855
Epoch: [35] Val - EpochT: 0.2 min, Loss: 0.1170
Global IOU: 0.8383
Epoch: [36] Train - EpochT: 0.1 min, BatchT: 0.192s, DataT: 0.024s, Loss: 0.3603
Epoch: [36] Val - EpochT: 0.2 min, Loss: 0.1166
Global IOU: 0.8367
Epoch: [37] Train - EpochT: 0.1 min, BatchT: 0.191s, DataT: 0.023s, Loss: 0.3626
Epoch: [37] Val - EpochT: 0.2 min, Loss: 0.1283
Global IOU: 0.8120
Epoch: [38] Train - EpochT: 0.1 min, BatchT: 0.188s, DataT: 0.021s, Loss: 0.4115
Epoch: [38] Val - EpochT: 0.2 min, Loss: 0.1142
Global IOU: 0.8338
Epoch: [39] Train - EpochT: 0.1 min, BatchT: 0.192s, DataT: 0.026s, Loss: 0.4084
Epoch: [39] Val - EpochT: 0.2 min, Loss: 0.1132
Global IOU: 0.8361
Epoch: [40] Train - EpochT: 0.1 min, BatchT: 0.193s, DataT: 0.026s, Loss: 0.3824
Epoch: [40] Val - EpochT: 0.2 min, Loss: 0.1136
Global IOU: 0.8342
Epoch 00030: reducing learning rate of group 0 to 6.2500e-06.
Epoch: [41] Train - EpochT: 0.1 min, BatchT: 0.195s, DataT: 0.024s, Loss: 0.3751
Epoch: [41] Val - EpochT: 0.2 min, Loss: 0.1139
Global IOU: 0.8347
Epoch: [42] Train - EpochT: 0.1 min, BatchT: 0.194s, DataT: 0.025s, Loss: 0.3851
Epoch: [42] Val - EpochT: 0.2 min, Loss: 0.1155
Global IOU: 0.8349
Epoch: [43] Train - EpochT: 0.1 min, BatchT: 0.192s, DataT: 0.021s, Loss: 0.3795
Epoch: [43] Val - EpochT: 0.2 min, Loss: 0.1145
Global IOU: 0.8301
Epoch: [44] Train - EpochT: 0.2 min, BatchT: 0.197s, DataT: 0.024s, Loss: 0.3585
Epoch: [44] Val - EpochT: 0.2 min, Loss: 0.1134
Global IOU: 0.8400
Epoch: [45] Train - EpochT: 0.1 min, BatchT: 0.189s, DataT: 0.022s, Loss: 0.3709
Epoch: [45] Val - EpochT: 0.2 min, Loss: 0.1158
Global IOU: 0.8323
Epoch: [46] Train - EpochT: 0.1 min, BatchT: 0.190s, DataT: 0.022s, Loss: 0.3615
Epoch: [46] Val - EpochT: 0.2 min, Loss: 0.1148
Global IOU: 0.8345
Epoch: [47] Train - EpochT: 0.1 min, BatchT: 0.195s, DataT: 0.027s, Loss: 0.3540
Epoch: [47] Val - EpochT: 0.2 min, Loss: 0.1148
Global IOU: 0.8332
Epoch 00037: reducing learning rate of group 0 to 3.1250e-06.
Epoch: [48] Train - EpochT: 0.1 min, BatchT: 0.195s, DataT: 0.021s, Loss: 0.3747
Epoch: [48] Val - EpochT: 0.2 min, Loss: 0.1132
Global IOU: 0.8396
Epoch: [49] Train - EpochT: 0.1 min, BatchT: 0.186s, DataT: 0.022s, Loss: 0.3861
Epoch: [49] Val - EpochT: 0.2 min, Loss: 0.1119
Global IOU: 0.8415
Epoch: [50] Train - EpochT: 0.1 min, BatchT: 0.187s, DataT: 0.022s, Loss: 0.3728
Epoch: [50] Val - EpochT: 0.2 min, Loss: 0.1132
Global IOU: 0.8382
Epoch: [51] Train - EpochT: 0.1 min, BatchT: 0.189s, DataT: 0.024s, Loss: 0.3743
Epoch: [51] Val - EpochT: 0.2 min, Loss: 0.1132
Global IOU: 0.8401
Epoch: [52] Train - EpochT: 0.1 min, BatchT: 0.192s, DataT: 0.023s, Loss: 0.3684
Epoch: [52] Val - EpochT: 0.2 min, Loss: 0.1120
Global IOU: 0.8422
Epoch: [53] Train - EpochT: 0.1 min, BatchT: 0.196s, DataT: 0.024s, Loss: 0.3556
Epoch: [53] Val - EpochT: 0.2 min, Loss: 0.1161
Global IOU: 0.8391
Epoch: [54] Train - EpochT: 0.1 min, BatchT: 0.195s, DataT: 0.023s, Loss: 0.3517
Epoch: [54] Val - EpochT: 0.2 min, Loss: 0.1125
Global IOU: 0.8417
Epoch 00044: reducing learning rate of group 0 to 1.5625e-06.
Epoch: [55] Train - EpochT: 0.2 min, BatchT: 0.209s, DataT: 0.023s, Loss: 0.3888
Epoch: [55] Val - EpochT: 0.2 min, Loss: 0.1134
Global IOU: 0.8394
Epoch: [56] Train - EpochT: 0.2 min, BatchT: 0.206s, DataT: 0.025s, Loss: 0.3857
Epoch: [56] Val - EpochT: 0.2 min, Loss: 0.1170
Global IOU: 0.8390
Epoch: [57] Train - EpochT: 0.2 min, BatchT: 0.215s, DataT: 0.023s, Loss: 0.3651
Epoch: [57] Val - EpochT: 0.2 min, Loss: 0.1114
Global IOU: 0.8418
Epoch: [58] Train - EpochT: 0.2 min, BatchT: 0.214s, DataT: 0.021s, Loss: 0.3699
Epoch: [58] Val - EpochT: 0.2 min, Loss: 0.1136
Global IOU: 0.8404
Epoch: [59] Train - EpochT: 0.1 min, BatchT: 0.194s, DataT: 0.020s, Loss: 0.3682
Epoch: [59] Val - EpochT: 0.2 min, Loss: 0.1139
Global IOU: 0.8426
Epoch: [60] Train - EpochT: 0.1 min, BatchT: 0.191s, DataT: 0.020s, Loss: 0.3517
Epoch: [60] Val - EpochT: 0.2 min, Loss: 0.1125
Global IOU: 0.8421
Epoch: [61] Train - EpochT: 0.2 min, BatchT: 0.199s, DataT: 0.026s, Loss: 0.3749
Epoch: [61] Val - EpochT: 0.2 min, Loss: 0.1123
Global IOU: 0.8426
Epoch 00051: reducing learning rate of group 0 to 7.8125e-07.
Epoch: [62] Train - EpochT: 0.2 min, BatchT: 0.210s, DataT: 0.021s, Loss: 0.3689
Epoch: [62] Val - EpochT: 0.2 min, Loss: 0.1114
Global IOU: 0.8441
Epoch: [63] Train - EpochT: 0.2 min, BatchT: 0.199s, DataT: 0.024s, Loss: 0.3890
Epoch: [63] Val - EpochT: 0.2 min, Loss: 0.1104
Global IOU: 0.8413
Epoch: [64] Train - EpochT: 0.2 min, BatchT: 0.219s, DataT: 0.023s, Loss: 0.4220
Epoch: [64] Val - EpochT: 0.2 min, Loss: 0.1133
Global IOU: 0.8407
Epoch: [65] Train - EpochT: 0.2 min, BatchT: 0.199s, DataT: 0.022s, Loss: 0.3711
Epoch: [65] Val - EpochT: 0.2 min, Loss: 0.1098
Global IOU: 0.8427
Epoch: [66] Train - EpochT: 0.2 min, BatchT: 0.198s, DataT: 0.023s, Loss: 0.4122
Epoch: [66] Val - EpochT: 0.2 min, Loss: 0.1106
Global IOU: 0.8428
Epoch: [67] Train - EpochT: 0.2 min, BatchT: 0.207s, DataT: 0.022s, Loss: 0.3717
Epoch: [67] Val - EpochT: 0.2 min, Loss: 0.1116
Global IOU: 0.8428
Epoch: [68] Train - EpochT: 0.2 min, BatchT: 0.207s, DataT: 0.029s, Loss: 0.3668
Epoch: [68] Val - EpochT: 0.2 min, Loss: 0.1101
Global IOU: 0.8437
Epoch 00058: reducing learning rate of group 0 to 3.9063e-07.
Epoch: [69] Train - EpochT: 0.1 min, BatchT: 0.194s, DataT: 0.023s, Loss: 0.3685
Epoch: [69] Val - EpochT: 0.2 min, Loss: 0.1137
Global IOU: 0.8430
Epoch: [70] Train - EpochT: 0.2 min, BatchT: 0.216s, DataT: 0.023s, Loss: 0.3904
Epoch: [70] Val - EpochT: 0.2 min, Loss: 0.1132
Global IOU: 0.8443
Epoch: [71] Train - EpochT: 0.1 min, BatchT: 0.191s, DataT: 0.019s, Loss: 0.3572
Epoch: [71] Val - EpochT: 0.2 min, Loss: 0.1112
Global IOU: 0.8440
Epoch: [72] Train - EpochT: 0.1 min, BatchT: 0.194s, DataT: 0.019s, Loss: 0.3824
Epoch: [72] Val - EpochT: 0.2 min, Loss: 0.1113
Global IOU: 0.8436
Epoch: [73] Train - EpochT: 0.2 min, BatchT: 0.203s, DataT: 0.023s, Loss: 0.3830
Epoch: [73] Val - EpochT: 0.2 min, Loss: 0.1123
Global IOU: 0.8431
Epoch: [74] Train - EpochT: 0.1 min, BatchT: 0.196s, DataT: 0.023s, Loss: 0.3853
Epoch: [74] Val - EpochT: 0.2 min, Loss: 0.1106
Global IOU: 0.8440
Epoch: [75] Train - EpochT: 0.1 min, BatchT: 0.195s, DataT: 0.023s, Loss: 0.4105
Epoch: [75] Val - EpochT: 0.2 min, Loss: 0.1118
Global IOU: 0.8424
Epoch 00065: reducing learning rate of group 0 to 1.9531e-07.
Epoch: [76] Train - EpochT: 0.2 min, BatchT: 0.197s, DataT: 0.025s, Loss: 0.3825
Epoch: [76] Val - EpochT: 0.2 min, Loss: 0.1119
Global IOU: 0.8445
Epoch: [77] Train - EpochT: 0.2 min, BatchT: 0.210s, DataT: 0.023s, Loss: 0.3442
Epoch: [77] Val - EpochT: 0.2 min, Loss: 0.1124
Global IOU: 0.8425
Epoch: [78] Train - EpochT: 0.2 min, BatchT: 0.196s, DataT: 0.026s, Loss: 0.3657
Epoch: [78] Val - EpochT: 0.2 min, Loss: 0.1095
Global IOU: 0.8449
Epoch: [79] Train - EpochT: 0.2 min, BatchT: 0.197s, DataT: 0.023s, Loss: 0.4294
Epoch: [79] Val - EpochT: 0.2 min, Loss: 0.1149
Global IOU: 0.8406
Epoch: [80] Train - EpochT: 0.2 min, BatchT: 0.201s, DataT: 0.023s, Loss: 0.3634
Epoch: [80] Val - EpochT: 0.2 min, Loss: 0.1090
Global IOU: 0.8456
Epoch: [81] Train - EpochT: 0.2 min, BatchT: 0.209s, DataT: 0.024s, Loss: 0.3685
Epoch: [81] Val - EpochT: 0.2 min, Loss: 0.1114
Global IOU: 0.8441
Epoch: [82] Train - EpochT: 0.1 min, BatchT: 0.196s, DataT: 0.024s, Loss: 0.3542
Epoch: [82] Val - EpochT: 0.2 min, Loss: 0.1102
Global IOU: 0.8438
Epoch 00072: reducing learning rate of group 0 to 9.7656e-08.
Epoch: [83] Train - EpochT: 0.2 min, BatchT: 0.206s, DataT: 0.023s, Loss: 0.3720
Epoch: [83] Val - EpochT: 0.2 min, Loss: 0.1155
Global IOU: 0.8391
Epoch: [84] Train - EpochT: 0.2 min, BatchT: 0.199s, DataT: 0.027s, Loss: 0.4009
Epoch: [84] Val - EpochT: 0.2 min, Loss: 0.1113
Global IOU: 0.8426
Epoch: [85] Train - EpochT: 0.1 min, BatchT: 0.193s, DataT: 0.022s, Loss: 0.3783
Epoch: [85] Val - EpochT: 0.2 min, Loss: 0.1108
Global IOU: 0.8443
Epoch: [86] Train - EpochT: 0.2 min, BatchT: 0.197s, DataT: 0.026s, Loss: 0.3649
Epoch: [86] Val - EpochT: 0.2 min, Loss: 0.1105
Global IOU: 0.8454
Epoch: [87] Train - EpochT: 0.1 min, BatchT: 0.193s, DataT: 0.024s, Loss: 0.3943
Epoch: [87] Val - EpochT: 0.2 min, Loss: 0.1107
Global IOU: 0.8439
Epoch: [88] Train - EpochT: 0.2 min, BatchT: 0.198s, DataT: 0.026s, Loss: 0.3785
Epoch: [88] Val - EpochT: 0.2 min, Loss: 0.1127
Global IOU: 0.8418
Epoch: [89] Train - EpochT: 0.2 min, BatchT: 0.199s, DataT: 0.023s, Loss: 0.4026
Epoch: [89] Val - EpochT: 0.2 min, Loss: 0.1112
Global IOU: 0.8437
Epoch 00079: reducing learning rate of group 0 to 4.8828e-08.
Epoch: [90] Train - EpochT: 0.1 min, BatchT: 0.196s, DataT: 0.025s, Loss: 0.3802
Epoch: [90] Val - EpochT: 0.2 min, Loss: 0.1129
Global IOU: 0.8417
Epoch: [91] Train - EpochT: 0.1 min, BatchT: 0.194s, DataT: 0.025s, Loss: 0.3645
Epoch: [91] Val - EpochT: 0.2 min, Loss: 0.1096
Global IOU: 0.8442
Epoch: [92] Train - EpochT: 0.2 min, BatchT: 0.214s, DataT: 0.024s, Loss: 0.3711
Epoch: [92] Val - EpochT: 0.2 min, Loss: 0.1121
Global IOU: 0.8427
Epoch: [93] Train - EpochT: 0.2 min, BatchT: 0.213s, DataT: 0.023s, Loss: 0.3810
Epoch: [93] Val - EpochT: 0.2 min, Loss: 0.1128
Global IOU: 0.8435
Epoch: [94] Train - EpochT: 0.2 min, BatchT: 0.209s, DataT: 0.026s, Loss: 0.3919
Epoch: [94] Val - EpochT: 0.2 min, Loss: 0.1115
Global IOU: 0.8439
Epoch: [95] Train - EpochT: 0.1 min, BatchT: 0.191s, DataT: 0.021s, Loss: 0.3598
Epoch: [95] Val - EpochT: 0.2 min, Loss: 0.1119
Global IOU: 0.8431
Epoch: [96] Train - EpochT: 0.2 min, BatchT: 0.211s, DataT: 0.022s, Loss: 0.3643
Epoch: [96] Val - EpochT: 0.2 min, Loss: 0.1111
Global IOU: 0.8429
Epoch 00086: reducing learning rate of group 0 to 2.4414e-08.
Epoch: [97] Train - EpochT: 0.1 min, BatchT: 0.195s, DataT: 0.022s, Loss: 0.3621
Epoch: [97] Val - EpochT: 0.2 min, Loss: 0.1111
Global IOU: 0.8452
Epoch: [98] Train - EpochT: 0.1 min, BatchT: 0.192s, DataT: 0.027s, Loss: 0.3704
Epoch: [98] Val - EpochT: 0.2 min, Loss: 0.1118
Global IOU: 0.8436
Epoch: [99] Train - EpochT: 0.1 min, BatchT: 0.192s, DataT: 0.023s, Loss: 0.4019
Epoch: [99] Val - EpochT: 0.2 min, Loss: 0.1128
Global IOU: 0.8429
Epoch: [100] Train - EpochT: 0.2 min, BatchT: 0.201s, DataT: 0.024s, Loss: 0.3772
Epoch: [100] Val - EpochT: 0.2 min, Loss: 0.1117
Global IOU: 0.8440
Epoch: [101] Train - EpochT: 0.2 min, BatchT: 0.197s, DataT: 0.025s, Loss: 0.3551
Epoch: [101] Val - EpochT: 0.2 min, Loss: 0.1111
Global IOU: 0.8442
Epoch: [102] Train - EpochT: 0.1 min, BatchT: 0.196s, DataT: 0.023s, Loss: 0.3664
Epoch: [102] Val - EpochT: 0.2 min, Loss: 0.1115
Global IOU: 0.8435
Epoch: [103] Train - EpochT: 0.2 min, BatchT: 0.201s, DataT: 0.023s, Loss: 0.3493
Epoch: [103] Val - EpochT: 0.2 min, Loss: 0.1104
Global IOU: 0.8451
Epoch 00093: reducing learning rate of group 0 to 1.2207e-08.
Epoch: [104] Train - EpochT: 0.1 min, BatchT: 0.188s, DataT: 0.023s, Loss: 0.3805
Epoch: [104] Val - EpochT: 0.2 min, Loss: 0.1124
Global IOU: 0.8425
Epoch: [105] Train - EpochT: 0.1 min, BatchT: 0.190s, DataT: 0.023s, Loss: 0.3727
Epoch: [105] Val - EpochT: 0.2 min, Loss: 0.1102
Global IOU: 0.8441
Epoch: [106] Train - EpochT: 0.1 min, BatchT: 0.189s, DataT: 0.026s, Loss: 0.3863
Epoch: [106] Val - EpochT: 0.2 min, Loss: 0.1108
Global IOU: 0.8431
Epoch: [107] Train - EpochT: 0.2 min, BatchT: 0.198s, DataT: 0.033s, Loss: 0.3404
Epoch: [107] Val - EpochT: 0.2 min, Loss: 0.1103
Global IOU: 0.8441
Epoch: [108] Train - EpochT: 0.2 min, BatchT: 0.198s, DataT: 0.023s, Loss: 0.3867
Epoch: [108] Val - EpochT: 0.2 min, Loss: 0.1124
Global IOU: 0.8423
Epoch: [109] Train - EpochT: 0.2 min, BatchT: 0.198s, DataT: 0.030s, Loss: 0.3739
Epoch: [109] Val - EpochT: 0.2 min, Loss: 0.1115
Global IOU: 0.8438
Epoch: [110] Train - EpochT: 0.2 min, BatchT: 0.209s, DataT: 0.025s, Loss: 0.3866
Epoch: [110] Val - EpochT: 0.2 min, Loss: 0.1101
Global IOU: 0.8428
Epoch: [111] Train - EpochT: 0.1 min, BatchT: 0.192s, DataT: 0.022s, Loss: 0.3876
Epoch: [111] Val - EpochT: 0.2 min, Loss: 0.1117
Global IOU: 0.8439
Early Stopping
\Best IOU: 0.8455862043978483 at Epoch: 80
----------------------------------------
Training Completed:
Total Training Time: 25.938113975524903 minutes
