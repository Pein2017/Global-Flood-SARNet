Downloaded and cached PSPNet model with resnet101 encoder.
Using NPU: npu:3 ...

TRAIN_CROP_SIZE = 256
TARGET_SIZE = 256
NUM_WORKERS = 4
PIN_MEMORY = False
BATCH_SIZE = 128
EPS = 1e-07
EXPERIMENT_NAME = 'PSPNet-resnet101-b128'
ENCODER_NAME = 'resnet101'
MODEL_NAME = 'PSPNet'
ENCODER_WEIGHTS = 'imagenet'
IN_CHANNELS = 2
CLASSES = 2
PATIENCE = 6
N_EPOCHS = 200
LEARNING_RATE = 0.0001
EARLY_STOP_THRESHOLD = 10
EARLY_STOP_PATIENCE = 30

-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/-\|/Epoch: [1] Train - EpochT: 6.7 min, BatchT: 67.234s, DataT: 0.768s, Loss: 0.6407
-Epoch: [1] Val - EpochT: 6.9 min, Loss: 0.6792
Global IOU: 0.0029
Epoch: [2] Train - EpochT: 0.2 min, BatchT: 1.672s, DataT: 1.379s, Loss: 0.5428
Epoch: [2] Val - EpochT: 0.2 min, Loss: 0.6456
Global IOU: 0.0067
Epoch: [3] Train - EpochT: 0.2 min, BatchT: 1.694s, DataT: 1.401s, Loss: 0.5184
Epoch: [3] Val - EpochT: 0.2 min, Loss: 0.6022
Global IOU: 0.1443
Epoch: [4] Train - EpochT: 0.2 min, BatchT: 1.631s, DataT: 1.336s, Loss: 0.4724
Epoch: [4] Val - EpochT: 0.2 min, Loss: 0.5261
Global IOU: 0.5717
Epoch: [5] Train - EpochT: 0.2 min, BatchT: 1.614s, DataT: 1.322s, Loss: 0.4290
Epoch: [5] Val - EpochT: 0.2 min, Loss: 0.4045
Global IOU: 0.5896
Epoch: [6] Train - EpochT: 0.2 min, BatchT: 1.673s, DataT: 1.380s, Loss: 0.4608
Epoch: [6] Val - EpochT: 0.2 min, Loss: 0.3055
Global IOU: 0.5742
Epoch: [7] Train - EpochT: 0.2 min, BatchT: 1.659s, DataT: 1.367s, Loss: 0.4337
Epoch: [7] Val - EpochT: 0.2 min, Loss: 0.2563
Global IOU: 0.6391
Epoch: [8] Train - EpochT: 0.2 min, BatchT: 1.658s, DataT: 1.366s, Loss: 0.4553
Epoch: [8] Val - EpochT: 0.2 min, Loss: 0.2139
Global IOU: 0.6755
Epoch: [9] Train - EpochT: 0.2 min, BatchT: 1.690s, DataT: 1.396s, Loss: 0.4485
Epoch: [9] Val - EpochT: 0.2 min, Loss: 0.1945
Global IOU: 0.6800
Epoch: [10] Train - EpochT: 0.2 min, BatchT: 1.711s, DataT: 1.419s, Loss: 0.4531
Epoch: [10] Val - EpochT: 0.2 min, Loss: 0.2022
Global IOU: 0.6964
Epoch: [11] Train - EpochT: 0.2 min, BatchT: 1.726s, DataT: 1.434s, Loss: 0.4152
Epoch: [11] Val - EpochT: 0.2 min, Loss: 0.1878
Global IOU: 0.7061
Epoch: [12] Train - EpochT: 0.2 min, BatchT: 1.674s, DataT: 1.382s, Loss: 0.4292
Epoch: [12] Val - EpochT: 0.2 min, Loss: 0.2121
Global IOU: 0.7113
Epoch: [13] Train - EpochT: 0.2 min, BatchT: 1.729s, DataT: 1.437s, Loss: 0.4616
Epoch: [13] Val - EpochT: 0.2 min, Loss: 0.2478
Global IOU: 0.6027
Epoch: [14] Train - EpochT: 0.2 min, BatchT: 1.719s, DataT: 1.426s, Loss: 0.4240
Epoch: [14] Val - EpochT: 0.2 min, Loss: 0.2054
Global IOU: 0.6992
Epoch: [15] Train - EpochT: 0.2 min, BatchT: 1.762s, DataT: 1.470s, Loss: 0.4190
Epoch: [15] Val - EpochT: 0.2 min, Loss: 0.1987
Global IOU: 0.7211
Epoch: [16] Train - EpochT: 0.2 min, BatchT: 1.690s, DataT: 1.398s, Loss: 0.4445
Epoch: [16] Val - EpochT: 0.2 min, Loss: 0.2809
Global IOU: 0.4997
Epoch: [17] Train - EpochT: 0.2 min, BatchT: 1.740s, DataT: 1.447s, Loss: 0.4243
Epoch: [17] Val - EpochT: 0.2 min, Loss: 0.2206
Global IOU: 0.6878
Epoch: [18] Train - EpochT: 0.2 min, BatchT: 1.660s, DataT: 1.367s, Loss: 0.4656
Epoch: [18] Val - EpochT: 0.2 min, Loss: 0.1828
Global IOU: 0.7238
Epoch: [19] Train - EpochT: 0.2 min, BatchT: 1.662s, DataT: 1.369s, Loss: 0.4409
Epoch: [19] Val - EpochT: 0.2 min, Loss: 0.2581
Global IOU: 0.5704
Epoch: [20] Train - EpochT: 0.2 min, BatchT: 1.679s, DataT: 1.387s, Loss: 0.4246
Epoch: [20] Val - EpochT: 0.2 min, Loss: 0.1824
Global IOU: 0.7098
Epoch: [21] Train - EpochT: 0.2 min, BatchT: 1.660s, DataT: 1.367s, Loss: 0.4113
Epoch: [21] Val - EpochT: 0.2 min, Loss: 0.1826
Global IOU: 0.7128
Epoch: [22] Train - EpochT: 0.2 min, BatchT: 1.693s, DataT: 1.400s, Loss: 0.4674
Epoch: [22] Val - EpochT: 0.2 min, Loss: 0.1676
Global IOU: 0.7397
Epoch: [23] Train - EpochT: 0.2 min, BatchT: 1.662s, DataT: 1.369s, Loss: 0.4052
Epoch: [23] Val - EpochT: 0.2 min, Loss: 0.1748
Global IOU: 0.7241
Epoch 00013: reducing learning rate of group 0 to 5.0000e-05.
Epoch: [24] Train - EpochT: 0.2 min, BatchT: 1.661s, DataT: 1.369s, Loss: 0.4092
Epoch: [24] Val - EpochT: 0.2 min, Loss: 0.1676
Global IOU: 0.7320
Epoch: [25] Train - EpochT: 0.2 min, BatchT: 1.629s, DataT: 1.335s, Loss: 0.4312
Epoch: [25] Val - EpochT: 0.2 min, Loss: 0.1717
Global IOU: 0.7151
Epoch: [26] Train - EpochT: 0.2 min, BatchT: 1.696s, DataT: 1.403s, Loss: 0.4032
Epoch: [26] Val - EpochT: 0.2 min, Loss: 0.1735
Global IOU: 0.7178
Epoch: [27] Train - EpochT: 0.2 min, BatchT: 1.679s, DataT: 1.385s, Loss: 0.4318
Epoch: [27] Val - EpochT: 0.2 min, Loss: 0.1776
Global IOU: 0.7255
Epoch: [28] Train - EpochT: 0.2 min, BatchT: 1.624s, DataT: 1.332s, Loss: 0.4146
Epoch: [28] Val - EpochT: 0.2 min, Loss: 0.1714
Global IOU: 0.7338
Epoch: [29] Train - EpochT: 0.2 min, BatchT: 1.683s, DataT: 1.390s, Loss: 0.3934
Epoch: [29] Val - EpochT: 0.2 min, Loss: 0.1710
Global IOU: 0.7376
Epoch: [30] Train - EpochT: 0.2 min, BatchT: 1.681s, DataT: 1.388s, Loss: 0.3969
Epoch: [30] Val - EpochT: 0.2 min, Loss: 0.1703
Global IOU: 0.7400
Epoch 00020: reducing learning rate of group 0 to 2.5000e-05.
Epoch: [31] Train - EpochT: 0.2 min, BatchT: 1.640s, DataT: 1.348s, Loss: 0.4081
Epoch: [31] Val - EpochT: 0.2 min, Loss: 0.1645
Global IOU: 0.7424
Epoch: [32] Train - EpochT: 0.2 min, BatchT: 1.631s, DataT: 1.339s, Loss: 0.3890
Epoch: [32] Val - EpochT: 0.2 min, Loss: 0.1638
Global IOU: 0.7397
Epoch: [33] Train - EpochT: 0.2 min, BatchT: 1.725s, DataT: 1.432s, Loss: 0.4008
Epoch: [33] Val - EpochT: 0.2 min, Loss: 0.1688
Global IOU: 0.7303
Epoch: [34] Train - EpochT: 0.2 min, BatchT: 1.678s, DataT: 1.385s, Loss: 0.4171
Epoch: [34] Val - EpochT: 0.2 min, Loss: 0.1676
Global IOU: 0.7310
Epoch: [35] Train - EpochT: 0.2 min, BatchT: 1.651s, DataT: 1.358s, Loss: 0.3978
Epoch: [35] Val - EpochT: 0.2 min, Loss: 0.1618
Global IOU: 0.7391
Epoch: [36] Train - EpochT: 0.2 min, BatchT: 1.804s, DataT: 1.512s, Loss: 0.4113
Epoch: [36] Val - EpochT: 0.2 min, Loss: 0.1587
Global IOU: 0.7438
Epoch: [37] Train - EpochT: 0.2 min, BatchT: 1.650s, DataT: 1.358s, Loss: 0.3686
Epoch: [37] Val - EpochT: 0.2 min, Loss: 0.1588
Global IOU: 0.7453
Epoch 00027: reducing learning rate of group 0 to 1.2500e-05.
Epoch: [38] Train - EpochT: 0.2 min, BatchT: 1.660s, DataT: 1.367s, Loss: 0.3844
Epoch: [38] Val - EpochT: 0.2 min, Loss: 0.1569
Global IOU: 0.7475
Epoch: [39] Train - EpochT: 0.2 min, BatchT: 1.767s, DataT: 1.475s, Loss: 0.4178
Epoch: [39] Val - EpochT: 0.2 min, Loss: 0.1575
Global IOU: 0.7472
Epoch: [40] Train - EpochT: 0.2 min, BatchT: 1.680s, DataT: 1.386s, Loss: 0.3921
Epoch: [40] Val - EpochT: 0.2 min, Loss: 0.1583
Global IOU: 0.7474
Epoch: [41] Train - EpochT: 0.2 min, BatchT: 1.642s, DataT: 1.350s, Loss: 0.3916
Epoch: [41] Val - EpochT: 0.2 min, Loss: 0.1589
Global IOU: 0.7464
Epoch: [42] Train - EpochT: 0.2 min, BatchT: 1.699s, DataT: 1.406s, Loss: 0.3954
Epoch: [42] Val - EpochT: 0.2 min, Loss: 0.1601
Global IOU: 0.7448
Epoch: [43] Train - EpochT: 0.2 min, BatchT: 1.651s, DataT: 1.358s, Loss: 0.4042
Epoch: [43] Val - EpochT: 0.2 min, Loss: 0.1613
Global IOU: 0.7433
Epoch: [44] Train - EpochT: 0.2 min, BatchT: 1.658s, DataT: 1.365s, Loss: 0.4224
Epoch: [44] Val - EpochT: 0.2 min, Loss: 0.1630
Global IOU: 0.7427
Epoch 00034: reducing learning rate of group 0 to 6.2500e-06.
Epoch: [45] Train - EpochT: 0.2 min, BatchT: 1.700s, DataT: 1.408s, Loss: 0.3932
Epoch: [45] Val - EpochT: 0.2 min, Loss: 0.1655
Global IOU: 0.7402
Epoch: [46] Train - EpochT: 0.2 min, BatchT: 1.670s, DataT: 1.378s, Loss: 0.3753
Epoch: [46] Val - EpochT: 0.2 min, Loss: 0.1632
Global IOU: 0.7417
Epoch: [47] Train - EpochT: 0.2 min, BatchT: 1.658s, DataT: 1.366s, Loss: 0.3826
Epoch: [47] Val - EpochT: 0.2 min, Loss: 0.1591
Global IOU: 0.7461
Epoch: [48] Train - EpochT: 0.2 min, BatchT: 1.755s, DataT: 1.463s, Loss: 0.4495
Epoch: [48] Val - EpochT: 0.2 min, Loss: 0.1575
Global IOU: 0.7482
Epoch: [49] Train - EpochT: 0.2 min, BatchT: 1.670s, DataT: 1.377s, Loss: 0.3800
Epoch: [49] Val - EpochT: 0.2 min, Loss: 0.1561
Global IOU: 0.7506
Epoch: [50] Train - EpochT: 0.2 min, BatchT: 1.684s, DataT: 1.391s, Loss: 0.4246
Epoch: [50] Val - EpochT: 0.2 min, Loss: 0.1577
Global IOU: 0.7478
Epoch: [51] Train - EpochT: 0.2 min, BatchT: 1.640s, DataT: 1.347s, Loss: 0.3785
Epoch: [51] Val - EpochT: 0.2 min, Loss: 0.1573
Global IOU: 0.7488
Epoch 00041: reducing learning rate of group 0 to 3.1250e-06.
Epoch: [52] Train - EpochT: 0.2 min, BatchT: 1.651s, DataT: 1.359s, Loss: 0.3648
Epoch: [52] Val - EpochT: 0.2 min, Loss: 0.1578
Global IOU: 0.7479
Epoch: [53] Train - EpochT: 0.2 min, BatchT: 1.772s, DataT: 1.480s, Loss: 0.4334
Epoch: [53] Val - EpochT: 0.2 min, Loss: 0.1578
Global IOU: 0.7481
Epoch: [54] Train - EpochT: 0.2 min, BatchT: 1.678s, DataT: 1.386s, Loss: 0.3896
Epoch: [54] Val - EpochT: 0.2 min, Loss: 0.1596
Global IOU: 0.7448
Epoch: [55] Train - EpochT: 0.2 min, BatchT: 1.680s, DataT: 1.388s, Loss: 0.3953
Epoch: [55] Val - EpochT: 0.2 min, Loss: 0.1581
Global IOU: 0.7478
Epoch: [56] Train - EpochT: 0.2 min, BatchT: 1.740s, DataT: 1.448s, Loss: 0.3873
Epoch: [56] Val - EpochT: 0.2 min, Loss: 0.1575
Global IOU: 0.7475
Epoch: [57] Train - EpochT: 0.2 min, BatchT: 1.683s, DataT: 1.389s, Loss: 0.4174
Epoch: [57] Val - EpochT: 0.2 min, Loss: 0.1577
Global IOU: 0.7482
Epoch: [58] Train - EpochT: 0.2 min, BatchT: 1.720s, DataT: 1.428s, Loss: 0.3980
Epoch: [58] Val - EpochT: 0.2 min, Loss: 0.1572
Global IOU: 0.7485
Epoch 00048: reducing learning rate of group 0 to 1.5625e-06.
Epoch: [59] Train - EpochT: 0.2 min, BatchT: 1.653s, DataT: 1.360s, Loss: 0.4118
Epoch: [59] Val - EpochT: 0.2 min, Loss: 0.1568
Global IOU: 0.7493
Epoch: [60] Train - EpochT: 0.2 min, BatchT: 1.646s, DataT: 1.352s, Loss: 0.4166
Epoch: [60] Val - EpochT: 0.2 min, Loss: 0.1570
Global IOU: 0.7483
Epoch: [61] Train - EpochT: 0.2 min, BatchT: 1.803s, DataT: 1.509s, Loss: 0.3892
Epoch: [61] Val - EpochT: 0.2 min, Loss: 0.1562
Global IOU: 0.7496
Epoch: [62] Train - EpochT: 0.2 min, BatchT: 1.755s, DataT: 1.452s, Loss: 0.4089
Epoch: [62] Val - EpochT: 0.2 min, Loss: 0.1571
Global IOU: 0.7480
Epoch: [63] Train - EpochT: 0.2 min, BatchT: 1.739s, DataT: 1.436s, Loss: 0.4244
Epoch: [63] Val - EpochT: 0.2 min, Loss: 0.1575
Global IOU: 0.7468
Epoch: [64] Train - EpochT: 0.2 min, BatchT: 1.683s, DataT: 1.379s, Loss: 0.4169
Epoch: [64] Val - EpochT: 0.2 min, Loss: 0.1577
Global IOU: 0.7471
Epoch: [65] Train - EpochT: 0.2 min, BatchT: 1.634s, DataT: 1.330s, Loss: 0.3952
Epoch: [65] Val - EpochT: 0.2 min, Loss: 0.1584
Global IOU: 0.7463
Epoch 00055: reducing learning rate of group 0 to 7.8125e-07.
Epoch: [66] Train - EpochT: 0.2 min, BatchT: 1.750s, DataT: 1.445s, Loss: 0.3884
Epoch: [66] Val - EpochT: 0.2 min, Loss: 0.1567
Global IOU: 0.7485
Epoch: [67] Train - EpochT: 0.2 min, BatchT: 1.689s, DataT: 1.385s, Loss: 0.3964
Epoch: [67] Val - EpochT: 0.2 min, Loss: 0.1585
Global IOU: 0.7464
Epoch: [68] Train - EpochT: 0.2 min, BatchT: 1.675s, DataT: 1.371s, Loss: 0.4471
Epoch: [68] Val - EpochT: 0.2 min, Loss: 0.1570
Global IOU: 0.7483
Epoch: [69] Train - EpochT: 0.2 min, BatchT: 1.641s, DataT: 1.337s, Loss: 0.3951
Epoch: [69] Val - EpochT: 0.2 min, Loss: 0.1567
Global IOU: 0.7492
Epoch: [70] Train - EpochT: 0.2 min, BatchT: 1.705s, DataT: 1.400s, Loss: 0.3984
Epoch: [70] Val - EpochT: 0.2 min, Loss: 0.1586
Global IOU: 0.7461
Epoch: [71] Train - EpochT: 0.2 min, BatchT: 1.712s, DataT: 1.407s, Loss: 0.3982
Epoch: [71] Val - EpochT: 0.2 min, Loss: 0.1573
Global IOU: 0.7482
Epoch: [72] Train - EpochT: 0.2 min, BatchT: 1.726s, DataT: 1.423s, Loss: 0.3765
Epoch: [72] Val - EpochT: 0.2 min, Loss: 0.1589
Global IOU: 0.7458
Epoch 00062: reducing learning rate of group 0 to 3.9063e-07.
Epoch: [73] Train - EpochT: 0.2 min, BatchT: 1.708s, DataT: 1.404s, Loss: 0.3715
Epoch: [73] Val - EpochT: 0.2 min, Loss: 0.1566
Global IOU: 0.7493
Epoch: [74] Train - EpochT: 0.2 min, BatchT: 1.690s, DataT: 1.387s, Loss: 0.4088
Epoch: [74] Val - EpochT: 0.2 min, Loss: 0.1569
Global IOU: 0.7493
Epoch: [75] Train - EpochT: 0.2 min, BatchT: 1.717s, DataT: 1.413s, Loss: 0.4077
Epoch: [75] Val - EpochT: 0.2 min, Loss: 0.1569
Global IOU: 0.7487
Epoch: [76] Train - EpochT: 0.2 min, BatchT: 1.698s, DataT: 1.394s, Loss: 0.3786
Epoch: [76] Val - EpochT: 0.2 min, Loss: 0.1567
Global IOU: 0.7490
Epoch: [77] Train - EpochT: 0.2 min, BatchT: 1.749s, DataT: 1.443s, Loss: 0.4024
Epoch: [77] Val - EpochT: 0.2 min, Loss: 0.1570
Global IOU: 0.7480
Epoch: [78] Train - EpochT: 0.2 min, BatchT: 1.687s, DataT: 1.382s, Loss: 0.3834
Epoch: [78] Val - EpochT: 0.2 min, Loss: 0.1579
Global IOU: 0.7466
Epoch: [79] Train - EpochT: 0.2 min, BatchT: 1.789s, DataT: 1.485s, Loss: 0.3961
Epoch: [79] Val - EpochT: 0.2 min, Loss: 0.1562
Global IOU: 0.7495
Epoch 00069: reducing learning rate of group 0 to 1.9531e-07.
Epoch: [80] Train - EpochT: 0.2 min, BatchT: 1.700s, DataT: 1.396s, Loss: 0.4079
Epoch: [80] Val - EpochT: 0.2 min, Loss: 0.1561
Global IOU: 0.7504
Early Stopping
\Best IOU: 0.750579496930419 at Epoch: 49
----------------------------------------
Training Completed:
Total Training Time: 25.215421867370605 minutes
